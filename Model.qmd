---
title: "Model"
format: pdf
editor: visual
---

```{r}
library(readr)
library(tidyverse)
teams <- read_csv("data/nfl_teams.csv")
spread <- read_csv("data/spreadspoke_scores.csv")
```

```{r}
head(teams)
summary(teams)
```

```{r}
head(spread)
summary(spread)
```
```{r}
#Todo
#1. Filter all games before the 79/80 season
  #Note: not all games have weather
#2. Do we need to create new tables?
```

#EDA
```{r}
library(ggplot2)

ggplot(spread, aes(x = score_home)) +
  geom_histogram(bins = 30, fill = "steelblue") +
  labs(title = "Distribution of Home Team Scores")

ggplot(spread, aes(x = score_away)) +
  geom_histogram(bins = 30, fill = "tomato") +
  labs(title = "Distribution of Away Team Scores")
```
```{r}
# Calculate medians
median_home  <- median(spread$score_home, na.rm = TRUE)
median_away  <- median(spread$score_away, na.rm = TRUE)

cat("Median Home Score:", median_home, "\n")
cat("Median Away Score:", median_away, "\n")
```

```{r}
spread$score_diff <- spread$score_home - spread$score_away

ggplot(spread, aes(x = spread_favorite, y = score_diff)) +
  geom_point(alpha = 0.4) +
  labs(title = "Vegas Spread vs. Actual Score Difference")
```
```{r}
team_perf <- spread |>
  group_by(team_home) |>
  summarise(avg_diff = mean(score_home - score_away, na.rm = TRUE)) |>
  arrange(desc(avg_diff))

head(team_perf, 10)
```

#___________________________________________________________________________________________________________________________________________________________________________________________________________________

#Models

#Model 1: Favorite Covers (Log)
```{r}
library(dplyr)

# Filter out pushes
nfl_logit <- nfl %>%
  filter(spread_favorite_cover_result %in% c("Cover", "Did Not Cover")) %>%
  mutate(cover_binary = ifelse(spread_favorite_cover_result == "Cover", 1, 0))

# Train/test split (train up to 2012, test after)
train_logit <- nfl_logit %>% filter(schedule_season <= 2012)
test_logit  <- nfl_logit %>% filter(schedule_season >  2012)

# Fit logistic regression model
logit_model <- glm(
  cover_binary ~ 
      division_matchup +
      team_home_favorite +
      schedule_week +
      team_home_elo_pre + team_away_elo_pre +
      score_avg_pts_for_roll_lag.x + score_avg_pts_against_roll_lag.x +
      score_avg_pts_for_roll_lag.y + score_avg_pts_against_roll_lag.y +
      weather_cold + weather_wind_bad + weather_rain + weather_snow,
  data = train_logit,
  family = binomial(link = "logit")
)

summary(logit_model)

# Predict on test set
test_logit$pred_cover_prob <- predict(logit_model, newdata=test_logit, type="response")
test_logit$pred_cover <- ifelse(test_logit$pred_cover_prob > 0.5, "Cover", "Did Not Cover")

# Confusion matrix
table(Predicted = test_logit$pred_cover,
      Actual    = test_logit$spread_favorite_cover_result)
```

#LASSO Variable Selection
```{r}
# Replace this whole cell with the block below

library(glmnet)
library(dplyr)
# caret only used optionally for confusionMatrix at the end
if(!requireNamespace("caret", quietly = TRUE)) {
  message("Package 'caret' not installed; confusionMatrix() summary will be skipped.")
}

# ---------------------
# 0) Ensure train/test exist & create elo_diff in both
# ---------------------
# (assumes train_logit and test_logit already created earlier in your script)
train_logit <- train_logit %>% mutate(elo_diff = team_home_elo_pre - team_away_elo_pre)
test_logit  <- test_logit  %>% mutate(elo_diff = team_home_elo_pre - team_away_elo_pre)

# ---------------------
# 1) Build model matrix (training) for glmnet (no intercept column)
#    NOTE: include the candidate predictors you want LASSO to consider
# ---------------------
X_train <- model.matrix(
  cover_binary ~ elo_diff +
    score_avg_pts_for_roll_lag.x +
    score_avg_pts_against_roll_lag.x +
    score_avg_pts_for_roll_lag.y +
    score_avg_pts_against_roll_lag.y +
    weather_cold + weather_wind_bad + weather_rain + weather_snow,
  data = train_logit
)[,-1]  # drop the intercept column

y_train <- train_logit$cover_binary

# ---------------------
# 2) LASSO CV to find lambda
# ---------------------
set.seed(123)
cvfit <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, nfolds = 10)
plot(cvfit)  # optional

best_lambda <- cvfit$lambda.min
cat("Best lambda (lambda.min):", best_lambda, "\n")

# ---------------------
# 3) Fit glmnet at best lambda and extract non-zero coefficients
# ---------------------
lasso_best <- glmnet(X_train, y_train, family = "binomial", alpha = 1, lambda = best_lambda)

# coef() returns a sparse matrix; coerce to a dense matrix for easy extraction
coef_mat <- as.matrix(coef(lasso_best))   # rows = variable names, col = lambda (only 1)

# find non-zero entries (exclude intercept in the count below)
nz_logical <- coef_mat[,1] != 0
nz_names_all <- rownames(coef_mat)[nz_logical]

# separate intercept
nz_names_no_intercept <- setdiff(nz_names_all, "(Intercept)")

# CLEAN: If any variable names end with TRUE because of how model.matrix encoded them,
# remove the trailing "TRUE" (so weather_coldTRUE -> weather_cold)
clean_selected <- gsub("TRUE$", "", nz_names_no_intercept)

# unique & keep ordering
clean_selected <- unique(clean_selected)

# Print selected variables and count
cat("\nLASSO selected (raw names):\n"); print(nz_names_no_intercept)
cat("\nLASSO selected (cleaned names):\n"); print(clean_selected)
cat("\nNumber of variables selected by LASSO (excluding intercept):", length(clean_selected), "\n")

# Print coefficient values for the selected variables
selected_coefs <- coef_mat[nz_names_all, 1, drop = FALSE]
rownames(selected_coefs) <- nz_names_all
cat("\nSelected coefficients (including intercept):\n")
print(selected_coefs)

# ---------------------
# 4) Safety: fallback if nothing selected
# ---------------------
if(length(clean_selected) == 0){
  stop("LASSO returned zero predictors. Consider relaxing lambda or providing a fallback model.")
}

# ---------------------
# 5) Build final glm formula using cleaned variable names and fit GLM
#    NOTE: final_glm expects the actual columns in train_logit/test_logit to be named exactly
# ---------------------
final_formula <- as.formula(paste("cover_binary ~", paste(clean_selected, collapse = " + ")))
cat("\nFinal GLM formula to fit:\n"); print(final_formula)

final_glm <- glm(final_formula, data = train_logit, family = binomial)
cat("\nSummary of final GLM (selected predictors):\n")
print(summary(final_glm))

# ---------------------
# 6) Predict on test set
# ---------------------
# Ensure test set contains the same predictor columns (clean_selected)
missing_cols <- setdiff(clean_selected, names(test_logit))
if(length(missing_cols) > 0){
  stop("Test set is missing required predictors for final model: ", paste(missing_cols, collapse = ", "))
}

test_logit$pred_prob  <- predict(final_glm, newdata = test_logit, type = "response")
test_logit$pred_label <- ifelse(test_logit$pred_prob > 0.5, "Cover", "Did Not Cover")

# ---------------------
# 7) Confusion matrix
# ---------------------
cm <- table(Predicted = test_logit$pred_label,
            Actual    = test_logit$spread_favorite_cover_result)
cat("\nSimple confusion table:\n")
print(cm)

# Optional: pretty confusion matrix / stats via caret
if(requireNamespace("caret", quietly = TRUE)){
  act <- factor(test_logit$spread_favorite_cover_result, levels = c("Cover", "Did Not Cover"))
  pre <- factor(test_logit$pred_label, levels = c("Cover", "Did Not Cover"))
  cat("\ncaret::confusionMatrix()\n")
  print(caret::confusionMatrix(pre, act))
}
cat("\nLASSO Selected Variables:\n", clean_selected)
```
3. Why accuracy is still ~52%?

Because:
	•	The Vegas spread is efficient;
	•	Favorite-cover prediction is very hard;
	•	Models usually get 51–55% accuracy at best;
	•	Weather + strength metrics help, but cannot beat Vegas significantly.

You actually got:
	•	Accuracy ~0.527
	•	Balanced accuracy ~0.520

This is typical and acceptable for ATS prediction research.

The goal is not perfection — it’s interpretability + understanding which factors matter.

⸻

4. What improved after selection?

✔ Much more interpretable

No meaningless schedule-week noise.

✔ Coefficients now make football sense:
	•	Home offense ↑ → more likely to cover
	•	Away offense ↑ → less likely to cover
	•	Away defense bad → favorite more likely to cover
	•	Snow/wind/cold → lower-scoring games → often reduce favorite margins

✔ Less multicollinearity

LASSO prunes correlated predictors.

✔ Cleaner statistical significance

Your p-values now cluster around meaningful variables:
	•	home/away offensive stats
	•	away defensive stats
	•	weather
	•	etc.

Why team_home_favorite in particular gets removed: 
LASSO removes variables that don’t add unique predictive power. The Elo difference variable typically encodes expected winner/strength; once elo_diff is in the model, a simpler home_favorite flag is often redundant. LASSO will keep the stronger representation of the same concept and drop the weaker one.
⸻

⭐ 5. Interpretive Summary You Can Use in Your Paper

After applying LASSO variable selection, the model retained predictors related to team strength (Elo difference), recent offensive/defensive performance (rolling scoring averages), and adverse weather conditions.

These variables reflect well-documented determinants of scoring margin in the NFL.

The variable selection process removed schedule-week controls and division indicators, which did not provide predictive value once team strength and weather were included.

The resulting model is more parsimonious, interpretable, and consistent with football analytics literature

#Testing interaction terms
```{r}
# Interaction testing pipeline
library(dplyr)
library(pROC)

# ------------- sanity checks -------------
if(!exists("train_logit") || !exists("test_logit")) stop("train_logit and test_logit must exist. Create train/test split first.")

# ensure binary target is numeric 0/1
if(!("cover_binary" %in% names(train_logit))) stop("train_logit must contain cover_binary (0/1).")
train_logit <- train_logit %>% mutate(cover_binary = as.numeric(cover_binary))
test_logit  <- test_logit  %>% mutate(cover_binary = as.numeric(cover_binary))

# ------------- pick base predictors -------------
# Prefer previously-generated lists if present, otherwise use a default sensible set.
if(exists("final_vars_for_glm") && length(final_vars_for_glm) > 0) {
  base_vars <- final_vars_for_glm
} else if(exists("clean_selected") && length(clean_selected) > 0) {
  base_vars <- clean_selected
} else {
  base_vars <- c(
    "elo_diff",
    "score_avg_pts_for_roll_lag.x","score_avg_pts_against_roll_lag.x",
    "score_avg_pts_for_roll_lag.y","score_avg_pts_against_roll_lag.y",
    "weather_cold","weather_wind_bad","weather_rain","weather_snow"
  )
  message("Using fallback base_vars; consider setting final_vars_for_glm or running LASSO first.")
}

# Ensure base_vars exist in the training dataframe
miss <- setdiff(base_vars, names(train_logit))
if(length(miss)>0) stop("These base predictors are missing from train_logit: ", paste(miss, collapse = ", "))

base_formula_str <- paste("cover_binary ~", paste(base_vars, collapse = " + "))
base_formula <- as.formula(base_formula_str)
message("Base formula: ", base_formula_str)

# ------------- fit base model -------------
base_glm <- glm(base_formula, data = train_logit, family = binomial)
base_aic <- AIC(base_glm)

# compute base test AUC (if test present)
compute_auc_safe <- function(model, test_df) {
  tryCatch({
    preds <- predict(model, newdata = test_df, type = "response")
    roc_obj <- roc(test_df$cover_binary, preds, quiet = TRUE)
    as.numeric(auc(roc_obj))
  }, error = function(e) {
    NA_real_
  })
}
base_auc <- compute_auc_safe(base_glm, test_logit)

cat("Base model AIC:", round(base_aic,2), "  Base test AUC:", round(base_auc,4), "\n\n")

# ------------- define interactions to test -------------
interactions <- c(
  "weather_wind_bad:elo_diff",
  "weather_cold:score_avg_pts_for_roll_lag.x",
  "score_avg_pts_for_roll_lag.x:score_avg_pts_against_roll_lag.y",
  "elo_diff:score_avg_pts_for_roll_lag.y"
)


# ------------- loop over interactions and evaluate -------------
results <- data.frame(
  interaction = interactions,
  aic = NA_real_,
  delta_aic = NA_real_,
  auc = NA_real_,
  delta_auc = NA_real_,
  coef = NA_real_,
  se = NA_real_,
  p_value = NA_real_,
  keep = NA,
  stringsAsFactors = FALSE
)

for(i in seq_along(interactions)) {
  int_term <- interactions[i]
  formula_i <- as.formula(paste(base_formula_str, "+", int_term))
  cat("\nTesting interaction:", int_term, "\n")
  glm_i <- tryCatch(glm(formula_i, data = train_logit, family = binomial),
                    error = function(e) { warning("Model failed for interaction: ", int_term); return(NULL) })
  if(is.null(glm_i)) {
    results$aic[i] <- NA; results$auc[i] <- NA; results$delta_aic[i] <- NA; results$delta_auc[i] <- NA; results$keep[i] <- FALSE
    next
  }
  aic_i <- AIC(glm_i)
  auc_i <- compute_auc_safe(glm_i, test_logit)
  # attempt to extract interaction coefficient, se, p-value
  s <- summary(glm_i)
  coef_name_variants <- c(int_term, gsub(":", ":", int_term), int_term) # keep simple
  # find exact matching row in coefficients
  coef_rows <- rownames(s$coefficients)
  # matching is sometimes "weather_wind_bad:elo_diff" or "elo_diff:weather_wind_bad"; search both orders
  possible_names <- c(int_term, paste(rev(strsplit(int_term, ":")[[1]]), collapse=":" ))
  row_match <- intersect(possible_names, coef_rows)
  if(length(row_match)==0){
    # attempt partial match
    row_match <- coef_rows[grepl(gsub(":", ".*", int_term), coef_rows)]
  }
  if(length(row_match) >= 1) {
    r <- s$coefficients[row_match[1], ]
    coef_val <- r["Estimate"]; se_val <- r["Std. Error"]; p_val <- r["Pr(>|z|)"]
  } else {
    coef_val <- NA; se_val <- NA; p_val <- NA
  }
  # record results
  results$aic[i] <- aic_i
  results$delta_aic[i] <- aic_i - base_aic
  results$auc[i] <- auc_i
  results$delta_auc[i] <- ifelse(is.na(base_auc)|is.na(auc_i), NA, auc_i - base_auc)
  results$coef[i] <- coef_val
  results$se[i] <- se_val
  results$p_value[i] <- p_val
  # decision rule: keep if AIC decreases AND AUC increases by at least 0.005
  results$keep[i] <- (!is.na(results$delta_aic[i]) && results$delta_aic[i] < 0) &&
                     (!is.na(results$delta_auc[i]) && results$delta_auc[i] >= 0.005)
  cat(sprintf(" AIC(base)=%.2f  AIC(int)=%.2f  ΔAIC=%.2f | AUC(base)=%.4f  AUC(int)=%.4f  ΔAUC=%.4f\n",
              base_aic, aic_i, results$delta_aic[i], base_auc, auc_i, results$delta_auc[i]))
  cat(sprintf(" coef=%.4f  SE=%.4f  p=%.4f  -> Keep? %s\n",
              ifelse(is.na(coef_val), NA, coef_val),
              ifelse(is.na(se_val), NA, se_val),
              ifelse(is.na(p_val), NA, p_val),
              ifelse(results$keep[i], "YES", "NO")))
}

# ------------- summarize results -------------
cat("\n\nInteraction test summary:\n")
print(results)

kept <- results$interaction[which(results$keep)]
cat("\nInteractions to KEEP (by our rule):\n")
print(kept)

# ------------- if any kept, fit final model with them -------------
if(length(kept) > 0) {
  final_formula_with_kept <- as.formula(paste(base_formula_str, paste(kept, collapse = " + "), sep = " + "))
  cat("\nFitting final model with kept interactions:\n")
  print(final_formula_with_kept)
  final_glm2 <- glm(final_formula_with_kept, data = train_logit, family = binomial)
  cat("AIC final:", AIC(final_glm2), "  Test AUC final:", compute_auc_safe(final_glm2, test_logit), "\n")
  print(summary(final_glm2))
} else {
  cat("\nNo interactions met the keep criteria (ΔAIC < 0 and ΔAUC >= 0.005). No final interaction model fitted.\n")
}
```
Whether the favorite covers is not only about how strong they are, but also about how well the underdog’s offense has been performing recently.

This interaction materially improves model fit and predictive accuracy.

```{r}
library(ggplot2)
library(dplyr)

# 1. Create a grid of values for prediction
elo_seq <- seq(min(train_logit$elo_diff, na.rm=TRUE),
               max(train_logit$elo_diff, na.rm=TRUE),
               length.out = 100)

away_off_seq <- quantile(train_logit$score_avg_pts_for_roll_lag.y,
                         probs = c(0.10, 0.50, 0.90), na.rm=TRUE)

# Use these quantiles as "weak", "average", "strong" away offense
away_labels <- c("Weak (10th %ile)",
                 "Avg. (50th %ile)",
                 "Strong (90th %ile)")

plot_df <- expand.grid(
  elo_diff = elo_seq,
  score_avg_pts_for_roll_lag.y = away_off_seq
)

# For all other variables, hold at mean or reference level
# (This makes plot interpretable & isolates interaction effect)
plot_df$score_avg_pts_for_roll_lag.x <- mean(train_logit$score_avg_pts_for_roll_lag.x, na.rm=TRUE)
plot_df$score_avg_pts_against_roll_lag.y <- mean(train_logit$score_avg_pts_against_roll_lag.y, na.rm=TRUE)
plot_df$weather_rain <- FALSE
plot_df$weather_snow <- FALSE

# Use the interaction-capable model (the one that includes elo_diff:score_avg...y)
plot_df$pred_prob <- predict(final_glm2, newdata = plot_df, type = "response")

# Label offense tiers
plot_df$underdog_offense <- factor(plot_df$score_avg_pts_for_roll_lag.y,
                                   labels = away_labels)

# 2. ggplot interaction curves
ggplot(plot_df, aes(x = elo_diff, y = pred_prob,
                    color = underdog_offense)) +
  geom_line(size = 1.4) +
  labs(
    title = "Interaction Effect:\nElo Difference × Underdog Offensive Form",
    x = "Elo Difference (Home – Away)",
    y = "Predicted Probability of Favorite Covering",
    color = "Away Offense Strength"
  ) +
  theme_minimal(base_size = 14) +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "top")
```
The X-axis:

Elo Difference (Home – Away)
	•	Negative → Away team stronger
	•	Positive → Home team stronger (favorite)

The Three Lines:

Each line is the predicted probability that the favorite covers, given a different level of underdog (away team) offensive strength:
	1.	Weak Underdog Offense (green, 10th percentile)
	2.	Average Underdog Offense (orange, 50th percentile)
	3.	Strong Underdog Offense (purple, 90th percentile)

The Y-axis:

Probability that the favorite covers the point spread

The interaction plot reveals a critical nonlinearity in ATS outcomes and underscores the importance of the detected interaction effect in the final model. Specifically, the relationship between Elo difference and the probability that the favorite covers the spread depends strongly on the underdog’s recent offensive form. When the underdog offense is weak, Elo difference exhibits a large positive effect: stronger favorites are substantially more likely to cover. In contrast, when the underdog offense is average, Elo mismatch provides almost no predictive advantage, flattening the relationship entirely. Most strikingly, when the underdog offense is strong, even large Elo differences fail to translate into higher cover probability. This interaction effect—statistically significant and materially improving model fit—indicates that team-strength mismatches matter only when the underdog struggles offensively. Put differently, favorites cover primarily when opponents cannot generate sustained scoring drives, confirming the intuition that raw strength disparities are insufficient to explain ATS outcomes without accounting for offensive momentum on the opposing side.

#Final Log Model:
```{r}
# Final logistic model using LASSO-selected predictors + chosen interaction
# Assumes `train_logit` and `test_logit` already exist in the environment.

# minimal checks & ensure target is numeric 0/1
train_logit <- train_logit %>% mutate(cover_binary = as.numeric(cover_binary))
test_logit  <- test_logit  %>% mutate(cover_binary = as.numeric(cover_binary))

# formula using the LASSO-selected variables and the chosen interaction
final_formula_with_kept <- cover_binary ~
  elo_diff +
  score_avg_pts_for_roll_lag.x +
  score_avg_pts_for_roll_lag.y +
  score_avg_pts_against_roll_lag.y +
  weather_rain + 
  elo_diff:score_avg_pts_for_roll_lag.y

# fit the logistic model
final_glm <- glm(final_formula_with_kept, data = train_logit, family = binomial)

# summary (coefficients, p-values)
print(summary(final_glm))

# predict on test set (probabilities and 0.5 threshold class)
test_logit$pred_prob <- predict(final_glm, newdata = test_logit, type = "response")
test_logit$pred_label <- ifelse(test_logit$pred_prob > 0.5, "Cover", "Did Not Cover")

# confusion table
conf_table <- table(Predicted = test_logit$pred_label,
                    Actual    = test_logit$spread_favorite_cover_result)
print(conf_table)

# optional: AUC (if pROC installed)
if(requireNamespace("pROC", quietly = TRUE)){
  roc_obj <- pROC::roc(test_logit$cover_binary, test_logit$pred_prob, quiet = TRUE)
  cat("Test AUC:", round(as.numeric(pROC::auc(roc_obj)), 4), "\n")
}
```
```{r}
library(pROC)

roc_obj <- roc(test_logit$cover_binary, test_logit$pred_prob)

ggplot(data.frame(
  fpr = 1 - roc_obj$specificities,
  tpr = roc_obj$sensitivities
), aes(x=fpr, y=tpr)) +
  geom_line(color="steelblue", size=1.2) +
  geom_abline(linetype="dashed", color="gray50") +
  labs(title=paste("ROC Curve (AUC =", round(auc(roc_obj), 3), ")"),
       x="False Positive Rate",
       y="True Positive Rate") +
  theme_minimal(base_size = 14)
```


ROC curve lies only modestly above the diagonal reference line. This means:
	•	The model has some ability to rank games correctly (favorites who are more likely to cover tend to have higher predicted probabilities),
	•	But the separation between positive and negative cases is small.

4. Why this is still valuable

Despite low AUC, the model reveals meaningful predictors—especially the interaction between Elo difference and underdog offensive form—which reflects real football dynamics. The model is therefore interpretively useful even if not strongly discriminative.

This matches the central goal of your project:
understanding which factors drive ATS outcomes, not outperforming Vegas.

#Data Prep for Model 2
```{r}
# ---- Compute Elo + rolling averages and attach to games ----
# Paste & run this AFTER you've loaded `spread <- read_csv("data/spreadspoke_scores.csv")`

library(dplyr)
library(lubridate)
library(zoo)      # for rollapplyr
library(stringr)

# defensive checks
if(!exists("spread")) stop("Data frame `spread` not found. Run read_csv('data/spreadspoke_scores.csv') first.")

# 1) parse schedule_date (try several common formats)
spread2 <- spread %>%
  mutate(
    # try parsing a few frequent formats; fallback to original if NA
    schedule_date_parsed = parse_date_time(schedule_date,
                                           orders = c("Y-m-d", "Y/m/d", "m/d/Y", "d/m/Y", "mdY", "BdY", "Ymd", "ymd")),
    # also try ISO and a second attempt for odd strings
    schedule_date_parsed = if_else(is.na(schedule_date_parsed) & !is.na(schedule_date),
                                   as_datetime(tryCatch(as.POSIXct(schedule_date, tz = "UTC"), error = function(e) NA)),
                                   schedule_date_parsed)
  )

# If many dates are still NA, show a sample to help adjust formats
na_dates <- sum(is.na(spread2$schedule_date_parsed))
if(na_dates > 0){
  message("Warning: ", na_dates, " schedule_date rows failed to parse. Showing 10 raw schedule_date samples:")
  print(unique(head(spread$schedule_date, 50)))
  # proceed but user should inspect if many NA
}

# Use parsed date or fallback to schedule_season + week ordering if no date
if(all(is.na(spread2$schedule_date_parsed))){
  stop("No parseable schedule_date values. Please tell me the date format, or ensure schedule_date column exists.")
}

# 2) create chronological ordering and a simple game id
spread2 <- spread2 %>%
  arrange(schedule_date_parsed) %>%
  mutate(game_id = row_number())

# 3) Prepare a long team-level table with one row per team per game (so we can compute team rolling stats)
# for home games
home_long <- spread2 %>%
  transmute(
    game_id,
    date = schedule_date_parsed,
    season = schedule_season,
    team = team_home,
    opponent = team_away,
    is_home = TRUE,
    pts_for = as.numeric(score_home),
    pts_against = as.numeric(score_away),
    home_win = ifelse(score_home > score_away, 1, ifelse(score_home == score_away, 0.5, 0))
  )

# for away games
away_long <- spread2 %>%
  transmute(
    game_id,
    date = schedule_date_parsed,
    season = schedule_season,
    team = team_away,
    opponent = team_home,
    is_home = FALSE,
    pts_for = as.numeric(score_away),
    pts_against = as.numeric(score_home),
    home_win = ifelse(score_home > score_away, 1, ifelse(score_home == score_away, 0.5, 0))  # same indicator; used only for outcome
  )

team_games <- bind_rows(home_long, away_long) %>%
  arrange(team, date, game_id)   # ensure stable order within team

# 4) compute rolling averages per team (exclude current game using lag)
# Choose window size (e.g., 5 games). You can change this to 3 or 10 as desired.
n_roll <- 5

team_games <- team_games %>%
  group_by(team) %>%
  arrange(date, game_id) %>%
  mutate(
    # lag the pts_for so that rollapplyr computes on previous games only
    pts_for_lag = lag(pts_for),
    pts_against_lag = lag(pts_against),
    games_played = row_number() - 1,   # number of prior games
    score_avg_pts_for_roll = ifelse(games_played == 0, NA,
                                    zoo::rollapplyr(pts_for_lag, width = n_roll, FUN = mean, partial = TRUE, fill = NA)),
    score_avg_pts_against_roll = ifelse(games_played == 0, NA,
                                        zoo::rollapplyr(pts_against_lag, width = n_roll, FUN = mean, partial = TRUE, fill = NA))
  ) %>%
  ungroup()

# 5) pivot back to wide to attach home/away rolling features to each game in spread2
# extract home-side features
home_roll <- team_games %>%
  filter(is_home) %>%
  select(game_id,
         score_avg_pts_for_roll_home = score_avg_pts_for_roll,
         score_avg_pts_against_roll_home = score_avg_pts_against_roll)

away_roll <- team_games %>%
  filter(!is_home) %>%
  select(game_id,
         score_avg_pts_for_roll_away = score_avg_pts_for_roll,
         score_avg_pts_against_roll_away = score_avg_pts_against_roll)

# join to spread2
spread3 <- spread2 %>%
  left_join(home_roll, by = "game_id") %>%
  left_join(away_roll, by = "game_id") %>%
  # create variable names matching your prior model (.x = home, .y = away)
  mutate(
    score_avg_pts_for_roll_lag.x = score_avg_pts_for_roll_home,
    score_avg_pts_against_roll_lag.x = score_avg_pts_against_roll_home,
    score_avg_pts_for_roll_lag.y = score_avg_pts_for_roll_away,
    score_avg_pts_against_roll_lag.y = score_avg_pts_against_roll_away
  )

# 6) Simple Elo rating timeline (if you don't already have Elo columns)
# Initialize all teams to 1500 and iterate through games in chronological order
teams_list <- unique(c(spread3$team_home, spread3$team_away))
elo_init <- 1500
K <- 20   # learning rate; you can adjust (e.g., 20-30)

# initialize vector for team elos
elo_table <- setNames(rep(elo_init, length(teams_list)), teams_list)

# prepare containers for pre-game elos
pre_home_elo <- numeric(nrow(spread3))
pre_away_elo <- numeric(nrow(spread3))

for(i in seq_len(nrow(spread3))){
  r <- spread3[i, ]
  h <- as.character(r$team_home)
  a <- as.character(r$team_away)
  # record pre-game Elo
  Eh <- elo_table[h]
  Ea <- elo_table[a]
  pre_home_elo[i] <- Eh
  pre_away_elo[i] <- Ea

  # compute actual outcome for home: 1 win, 0 loss, 0.5 tie
  if(is.na(r$score_home) | is.na(r$score_away)){
    actual_home <- NA_real_
  } else {
    actual_home <- ifelse(r$score_home > r$score_away, 1, ifelse(r$score_home == r$score_away, 0.5, 0))
  }

  if(!is.na(actual_home)){
    # expected home win probability (Elo logistic)
    expected_home <- 1 / (1 + 10^((Ea - Eh)/400))
    # update (simple)
    elo_table[h] <- Eh + K * (actual_home - expected_home)
    elo_table[a] <- Ea + K * ((1 - actual_home) - (1 - expected_home))  # equivalent: -K*(actual_home - expected_home)
  }
}

# attach pre-game elos and elo_diff
spread3 <- spread3 %>%
  mutate(
    team_home_elo_pre = pre_home_elo,
    team_away_elo_pre = pre_away_elo,
    elo_diff = team_home_elo_pre - team_away_elo_pre
  )

# 7) Quick summaries & create train/test splits as you used earlier
# print head to confirm
cat("New columns added (sample):\n")
print(colnames(spread3)[ (ncol(spread3)-15):ncol(spread3) ])

# count missingness for the new predictors
new_preds <- c("team_home_elo_pre","team_away_elo_pre","elo_diff",
               "score_avg_pts_for_roll_lag.x","score_avg_pts_against_roll_lag.x",
               "score_avg_pts_for_roll_lag.y","score_avg_pts_against_roll_lag.y")
cat("\nMissing counts for new predictors:\n")
print(sapply(spread3[ , new_preds], function(x) sum(is.na(x))))

# attach to global environment as 'spread' (so later code that expects 'spread' sees new columns)
spread <- spread3

# create nfl / train_logit / test_logit like your earlier pipeline expects:
# assume column spread_favorite_cover_result exists; otherwise create a simple binary cover flag
if(!"spread_favorite_cover_result" %in% names(spread)){
  # create a simple cover flag: whether favorite covered based on spread_favorite and score_diff
  spread <- spread %>%
    mutate(score_diff = score_home - score_away,
           spread_favorite_cover_result = ifelse(score_diff >= spread_favorite, "Cover", "Did Not Cover"))
}

nfl <- spread  # keep naming consistent with your earlier notebook

nfl_logit <- nfl %>%
  filter(spread_favorite_cover_result %in% c("Cover", "Did Not Cover")) %>%
  mutate(cover_binary = ifelse(spread_favorite_cover_result == "Cover", 1, 0))

train_logit <- nfl_logit %>% filter(schedule_season <= 2012)
test_logit  <- nfl_logit %>% filter(schedule_season >  2012)

cat("\nCreated train_logit and test_logit: train rows:", nrow(train_logit), " test rows:", nrow(test_logit), "\n")

# sanity: show the first 6 rows of the features we created
print(dplyr::select(head(spread, 6),
                    schedule_date_parsed, team_home, team_away, score_home, score_away,
                    team_home_elo_pre, team_away_elo_pre, elo_diff,
                    score_avg_pts_for_roll_lag.x, score_avg_pts_against_roll_lag.x,
                    score_avg_pts_for_roll_lag.y, score_avg_pts_against_roll_lag.y))
```
#Model 2: Point Differential:
```{r}
# ---- GAM for score_diff: fit, summary, and publication-ready plots ----
library(mgcv)
library(dplyr)
library(ggplot2)
if(!requireNamespace("yardstick", quietly = TRUE)) install.packages("yardstick")
library(yardstick)

# Defensive checks
if(!exists("spread")) stop("spread not found. Run preprocessing cell first.")
# prefer train/test named 'train'/'test' (from earlier GAM code) or fall back to splitting
if(!exists("train") || !exists("test")) {
  if("schedule_season" %in% names(spread) && !all(is.na(spread$schedule_season))) {
    train <- spread %>% filter(schedule_season <= 2012)
    test  <- spread %>% filter(schedule_season > 2012)
  } else {
    set.seed(2025)
    idx <- sample(nrow(spread))
    train <- spread[idx <= 0.8*nrow(spread), ]
    test  <- spread[idx > 0.8*nrow(spread), ]
  }
}

# Ensure target exists
if(!"score_diff" %in% names(train)) {
  if(all(c("score_home","score_away") %in% names(train))){
    train <- train %>% mutate(score_diff = score_home - score_away)
    test  <- test  %>% mutate(score_diff = score_home - score_away)
  } else stop("score_diff (or score_home/score_away) not found.")
}

# Build candidate smooths depending on column availability (this keeps code robust)
has <- function(x) x %in% names(train)

smooth_terms <- c()
# core predictors often available: spread_favorite, elo_diff, rolling offense/defense
if(has("spread_favorite"))   smooth_terms <- c(smooth_terms, "s(spread_favorite, k = 30)")
if(has("elo_diff"))          smooth_terms <- c(smooth_terms, "s(elo_diff, k = 20)")
if(has("score_avg_pts_for_roll_lag.x"))  smooth_terms <- c(smooth_terms, "s(score_avg_pts_for_roll_lag.x, k = 15)")
if(has("score_avg_pts_for_roll_lag.y"))  smooth_terms <- c(smooth_terms, "s(score_avg_pts_for_roll_lag.y, k = 15)")
if(has("score_avg_pts_against_roll_lag.y")) smooth_terms <- c(smooth_terms, "s(score_avg_pts_against_roll_lag.y, k = 15)")
# weather / time
if(has("weather_temperature")) smooth_terms <- c(smooth_terms, "s(weather_temperature, k = 10)")
if(has("weather_wind_mph"))    smooth_terms <- c(smooth_terms, "s(weather_wind_mph, k = 10)")
if(has("weather_humidity"))    smooth_terms <- c(smooth_terms, "s(weather_humidity, k = 10)")
if(has("schedule_season"))     smooth_terms <- c(smooth_terms, "s(schedule_season, k = 8)")
# team random intercepts (compact "re" smooth)
if(has("team_home")) smooth_terms <- c(smooth_terms, "s(team_home, bs = 're')")
if(has("team_away")) smooth_terms <- c(smooth_terms, "s(team_away, bs = 're')")

# Optionally add a 2D tensor interaction (only if both elodiff & away offense exist)
include_tensor <- all(c("elo_diff","score_avg_pts_for_roll_lag.y") %in% names(train))
if(include_tensor){
  # use a modest k to avoid overfitting in 2D
  smooth_terms <- c(smooth_terms, "te(elo_diff, score_avg_pts_for_roll_lag.y, k = c(8,8))")
  message("Including 2D tensor te(elo_diff, score_avg_pts_for_roll_lag.y).")
}

if(length(smooth_terms) == 0) stop("No candidate predictors found for GAM. Check column names in `train`.")

# build formula
rhs <- paste(smooth_terms, collapse = " + ")
gam_formula <- as.formula(paste("score_diff ~", rhs))
message("GAM formula: ", deparse(gam_formula))

# Fit GAM (REML, select=TRUE to let mgcv penalize away unused smooths)
set.seed(2025)
gam_fit <- tryCatch(
  mgcv::gam(gam_formula, data = train, method = "REML", select = TRUE),
  error = function(e){
    message("Initial GAM fit error: ", conditionMessage(e), " — retrying with smaller k's.")
    # fallback: replace large k values with smaller defaults and retry
    f2 <- gsub("k = [0-9]+", "k = 8", deparse(gam_formula))
    mgcv::gam(as.formula(f2), data = train, method = "REML", select = TRUE)
  }
)

# Print concise summary
cat("\n=== GAM summary (concise) ===\n")
print(summary(gam_fit))

# Save deviance explained & adj R-sq for methods/results writeup
dev_explained <- round(summary(gam_fit)$dev.expl * 100, 2)
r_adj <- round(summary(gam_fit)$r.sq, 4)

# ---- Publication-ready smooth plots ----
# 1) base plot.gam with shading (single page). Use png/pdf in knitted report if needed.
par(mfrow = c(ceiling(length(gam_fit$smooth)/2), 2), mar = c(4,4,2,1))
plot(gam_fit, shade = TRUE, seWithMean = TRUE, pages = 1)
par(mfrow = c(1,1))

# 2) If tensor included, produce a contour / image plot of the interaction
if(include_tensor){
  # find which term index corresponds to the te(...) term
  v <- grep("^ti?\\(", sapply(gam_fit$smooth, function(s) s$label))
  if(length(v) > 0){
    # vis.gam provides nice contour / perspective plots
    try({
      vis.gam(gam_fit, view = c("elo_diff","score_avg_pts_for_roll_lag.y"),
              plot.type = "contour", color = "topo", main = "Interaction: elo_diff x away offense")
    }, silent = TRUE)
  }
}

# ---- Predictions & evaluation on TEST ----
test <- test %>% mutate(pred = predict(gam_fit, newdata = test, type = "response"))

rmse_val <- yardstick::rmse_vec(truth = test$score_diff, estimate = test$pred)
mae_val  <- yardstick::mae_vec(truth = test$score_diff, estimate = test$pred)

cat(sprintf("\nTest RMSE: %.3f ; Test MAE: %.3f ; Deviance explained: %s%% ; R-sq(adj): %s\n",
            rmse_val, mae_val, dev_explained, r_adj))

# Actual vs Predicted scatter (publication-ready ggplot)
p1 <- ggplot(test, aes(x = pred, y = score_diff)) +
  geom_point(alpha = 0.25, size = 1.5) +
  geom_smooth(method = "lm", se = TRUE, color = "darkred", size = 0.9) +
  labs(title = "GAM: Actual vs Predicted (test)",
       subtitle = paste0("RMSE=", round(rmse_val,2), "  MAE=", round(mae_val,2),
                         "  |  Deviance explained=", dev_explained, "%"),
       x = "Predicted score_diff",
       y = "Actual score_diff") +
  theme_minimal(base_size = 13)

print(p1)

# ---- Save objects for later use in report ----
assign("gam_fit_final", gam_fit, envir = .GlobalEnv)
assign("gam_test_preds", test, envir = .GlobalEnv)
```

#smooth effect of spread on score margin
```{r}
plot(gam_fit, select = 1, shade = TRUE, main = "Effect of Spread")
plot(gam_fit, select = 2, shade = TRUE, main = "Effect of Temperature")
plot(gam_fit, select = 3, shade = TRUE, main = "Effect of Wind")
plot(gam_fit, select = 4, shade = TRUE, main = "Effect of Humidity")
plot(gam_fit, select = 5, shade = TRUE, main = "Season Trend")
plot(gam_fit, select = 6, shade = TRUE, main = "Team (Home) Random Effect")
plot(gam_fit, select = 7, shade = TRUE, main = "Team (Away) Random Effect")
```


